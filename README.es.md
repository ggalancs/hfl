# hfl

[![Licencia: HRUL v1.0](https://img.shields.io/badge/Licencia-HRUL%20v1.0-blue.svg)](LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![CI](https://github.com/ggalancs/hfl/actions/workflows/ci.yml/badge.svg)](https://github.com/ggalancs/hfl/actions/workflows/ci.yml)
[![Cobertura](https://img.shields.io/badge/cobertura-81%25-brightgreen.svg)](https://github.com/ggalancs/hfl)

Ejecuta modelos de HuggingFace localmente como Ollama.

> **[English version](README.md)**

## Por qu√© HFL?

**Ollama tiene un cat√°logo curado de ~500 modelos. HuggingFace Hub tiene m√°s de 500,000.**

Si quieres ejecutar un modelo que no est√° en el cat√°logo de Ollama ‚Äî un fine-tune espec√≠fico, un lanzamiento reciente de un laboratorio peque√±o, un modelo de nicho ‚Äî tienes que descargar manualmente desde HuggingFace, convertir a GGUF con llama.cpp, cuantizar y configurar la inferencia. **HFL automatiza todo esto en un solo comando.**

| Caracter√≠stica | Ollama | HFL |
|----------------|--------|-----|
| Cat√°logo de modelos | ~500 curados | 500K+ (todo HF Hub) |
| Auto-conversi√≥n | No necesaria (pre-convertidos) | S√≠ (safetensors‚ÜíGGUF) |
| Facilidad de uso | Excelente | Buena |
| Compatible con API OpenAI | S√≠ | S√≠ |
| Compatible con API Ollama | Nativo | S√≠ (drop-in) |
| Multi-backend | solo llama.cpp | llama.cpp + Transformers + vLLM |
| Verificaci√≥n de licencia | No | S√≠ (5 niveles de riesgo) |
| Trazabilidad legal | No | S√≠ (log de procedencia) |
| Madurez | Alta (establecido) | Pre-alpha (v0.1.0) |

**HFL no compite con Ollama ‚Äî lo complementa.** Usa Ollama para modelos curados; usa HFL cuando necesites algo del ecosistema completo de HuggingFace.

## Caracter√≠sticas

- **CLI y API**: Interfaz CLI completa m√°s API REST compatible con OpenAI y Ollama
- **B√∫squeda de Modelos**: B√∫squeda paginada interactiva en HuggingFace Hub (como `more`)
- **M√∫ltiples Backends**: llama.cpp (GGUF/CPU), Transformers (GPU nativo), vLLM (producci√≥n)
- **Conversi√≥n Autom√°tica**: Descarga modelos de HuggingFace y convierte a GGUF autom√°ticamente
- **Cuantizaci√≥n Inteligente**: Soporta niveles de cuantizaci√≥n Q2_K hasta F16
- **Compatible Drop-in**: Funciona como reemplazo de Ollama con herramientas existentes
- **Internacionalizado**: Soporte completo i18n (Ingl√©s, Espa√±ol) - configura `HFL_LANG` para cambiar idioma

## C√≥mo Funciona

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           Arquitectura de HFL                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  hfl pull   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ  HuggingFace Hub ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ  Almacenamiento ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ         ‚îÇ                  ‚îÇ         ‚îÇ   ~/.hfl/       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ  ‚Ä¢ API B√∫squeda  ‚îÇ         ‚îÇ   ‚îú‚îÄ‚îÄ models/   ‚îÇ   ‚îÇ
‚îÇ        ‚îÇ                 ‚îÇ  ‚Ä¢ Descarga      ‚îÇ         ‚îÇ   ‚îú‚îÄ‚îÄ cache/    ‚îÇ   ‚îÇ
‚îÇ        ‚îÇ                 ‚îÇ  ‚Ä¢ Info licencia ‚îÇ         ‚îÇ   ‚îî‚îÄ‚îÄ registry  ‚îÇ   ‚îÇ
‚îÇ        ‚ñº                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                              ‚îÇ             ‚îÇ
‚îÇ  ‚îÇ  Conversor  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ                                                            ‚îÇ
‚îÇ  ‚îÇ safetensors ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ GGUF (cuantizado Q2_K...F16)                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                            ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  hfl run    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ Motor Inferencia ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ      Chat       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ         ‚îÇ                  ‚îÇ         ‚îÇ   Interactivo   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ  ‚Ä¢ llama.cpp     ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                          ‚îÇ  ‚Ä¢ Transformers  ‚îÇ                               ‚îÇ
‚îÇ                          ‚îÇ  ‚Ä¢ vLLM          ‚îÇ                               ‚îÇ
‚îÇ                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  hfl serve  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ   API REST       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ  OpenAI SDK /   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ         ‚îÇ                  ‚îÇ         ‚îÇ clientes Ollama ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ  ‚Ä¢ /v1/chat/...  ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                          ‚îÇ  ‚Ä¢ /api/chat     ‚îÇ                               ‚îÇ
‚îÇ                          ‚îÇ  ‚Ä¢ /api/generate ‚îÇ                               ‚îÇ
‚îÇ                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Resumen del Flujo:**
1. **Pull**: Descargar de HuggingFace Hub ‚Üí Convertir a GGUF (si es necesario) ‚Üí Almacenar localmente
2. **Run**: Cargar modelo en motor de inferencia ‚Üí Iniciar sesi√≥n de chat interactivo
3. **Serve**: Iniciar servidor API ‚Üí Aceptar peticiones compatibles con OpenAI/Ollama

## Prerrequisitos

- **Python 3.10+** (requerido)
- **git** (para clonar llama.cpp durante la primera conversi√≥n)
- **cmake** y **compilador C++** (para compilar herramientas de cuantizaci√≥n de llama.cpp)
  - macOS: `xcode-select --install`
  - Ubuntu/Debian: `sudo apt install build-essential cmake`
  - Windows: Instalar Visual Studio Build Tools

> **Nota:** Las herramientas de compilaci√≥n solo son necesarias si conviertes modelos safetensors a GGUF. Si solo usas modelos GGUF pre-cuantizados, no son necesarias.

## Instalaci√≥n

```bash
# Instalaci√≥n b√°sica (CPU + GGUF)
pip install hfl

# Con soporte GPU (Transformers + bitsandbytes)
pip install "hfl[transformers]"

# Con vLLM para producci√≥n
pip install "hfl[vllm]"

# Todo incluido
pip install "hfl[all]"
```

## Inicio R√°pido

### Descargar un Modelo

```bash
# Descargar con cuantizaci√≥n Q4_K_M por defecto
hfl pull meta-llama/Llama-3.3-70B-Instruct

# Especificar nivel de cuantizaci√≥n
hfl pull meta-llama/Llama-3.3-70B-Instruct --quantize Q5_K_M

# Mantener como safetensors (para inferencia GPU)
hfl pull mistralai/Mistral-7B-Instruct-v0.3 --format safetensors

# Descargar con un alias personalizado para referencia m√°s f√°cil
hfl pull meta-llama/Llama-3.3-70B-Instruct --alias llama70b
```

### Chat Interactivo

```bash
# Iniciar chat con un modelo
hfl run llama-3.3-70b-instruct-q4_k_m

# Con prompt de sistema
hfl run llama-3.3-70b-instruct-q4_k_m --system "Eres un experto en Python"
```

### Servidor API

```bash
# Iniciar servidor (puerto 11434 por defecto, igual que Ollama)
hfl serve

# Pre-cargar un modelo
hfl serve --model llama-3.3-70b-instruct-q4_k_m

# Host/puerto personalizado
hfl serve --host 0.0.0.0 --port 8080
```

### Buscar Modelos en HuggingFace

```bash
# Buscar modelos (paginado como 'more')
hfl search llama

# Buscar solo modelos con archivos GGUF
hfl search mistral --gguf

# Personalizar paginaci√≥n y resultados
hfl search phi --limit 50 --page-size 5

# Ordenar por likes en vez de descargas
hfl search qwen --sort likes
```

**Controles de navegaci√≥n:**
- `ESPACIO` / `ENTER` - P√°gina siguiente
- `p` - P√°gina anterior
- `q` / `ESC` - Salir

### Gesti√≥n de Modelos

```bash
# Listar todos los modelos locales
hfl list

# Mostrar detalles del modelo
hfl inspect llama-3.3-70b-instruct-q4_k_m

# Eliminar un modelo
hfl rm llama-3.3-70b-instruct-q4_k_m

# Establecer un alias para un modelo existente
hfl alias llama-3.3-70b-instruct-q4_k_m llama70b

# Ahora usa el alias en cualquier comando
hfl run llama70b
hfl inspect llama70b
```

## Endpoints de API

### Compatible con OpenAI

```bash
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3.3-70b-instruct-q4_k_m",
    "messages": [{"role": "user", "content": "Hola!"}]
  }'
```

### Compatible con Ollama

```bash
curl http://localhost:11434/api/chat \
  -d '{
    "model": "llama-3.3-70b-instruct-q4_k_m",
    "messages": [{"role": "user", "content": "Hola!"}]
  }'
```

### Usando OpenAI Python SDK

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="no-necesario"
)

response = client.chat.completions.create(
    model="llama-3.3-70b-instruct-q4_k_m",
    messages=[{"role": "user", "content": "Explica la computaci√≥n cu√°ntica"}],
)
print(response.choices[0].message.content)
```

## Niveles de Cuantizaci√≥n

| Nivel | Bits/peso | Calidad | Caso de Uso |
|-------|-----------|---------|-------------|
| Q2_K | ~2.5 | ~80% | Compresi√≥n extrema |
| Q3_K_M | ~3.5 | ~87% | RAM baja |
| **Q4_K_M** | ~4.5 | ~92% | **Por defecto - mejor balance** |
| Q5_K_M | ~5.0 | ~96% | Alta calidad |
| Q6_K | ~6.5 | ~97% | Premium |
| Q8_0 | ~8.0 | ~98%+ | M√°xima calidad cuantizada |
| F16 | 16.0 | 100% | Sin cuantizaci√≥n |

## Requisitos de RAM

```
RAM necesaria ‚âà (par√°metros √ó bits_por_peso) / 8 + 2GB overhead

Ejemplo: Llama 3.3 70B con Q4_K_M
= (70B √ó 4.5) / 8 + 2GB ‚âà 41.4 GB
```

| Tama√±o Modelo | RAM Q4_K_M | Hardware Recomendado |
|---------------|------------|----------------------|
| 7B | ~5 GB | 8 GB RAM |
| 13B | ~9 GB | 16 GB RAM |
| 30B | ~20 GB | 32 GB RAM |
| 70B | ~42 GB | 48 GB+ RAM o GPU |

## Autenticaci√≥n

Configura tu token de HuggingFace para descargas m√°s r√°pidas y acceso a modelos restringidos:

```bash
# Login interactivo (recomendado - almacena token de forma segura)
hfl login

# O usa variable de entorno (m√°s privado - no persistido)
export HF_TOKEN=hf_tu_token_aqui
```

Obt√©n tu token en: https://huggingface.co/settings/tokens

## Configuraci√≥n

Variables de entorno:
- `HFL_HOME`: Directorio de datos (por defecto: `~/.hfl`)
- `HF_TOKEN`: Token de HuggingFace para modelos restringidos (alternativa a `hfl login`)
- `HFL_LANG`: Idioma de la interfaz (`en` para Ingl√©s, `es` para Espa√±ol). Por defecto Ingl√©s.

### Soporte de Idiomas

hfl soporta m√∫ltiples idiomas. Configura la variable de entorno `HFL_LANG` para cambiar el idioma del CLI:

```bash
# Usar Espa√±ol
export HFL_LANG=es
hfl --help

# Usar Ingl√©s (por defecto)
export HFL_LANG=en
hfl --help
```

Idiomas soportados: Ingl√©s (`en`), Espa√±ol (`es`)

## Limitaciones Conocidas

Esta es una versi√≥n v0.1.0. Las limitaciones conocidas incluyen:

- **Backend vLLM es experimental**: Implementaci√≥n b√°sica sin soporte completo de streaming
- **Sin limitaci√≥n de tasa en API**: Solo las llamadas a HuggingFace Hub tienen l√≠mite de tasa
- **CORS es permisivo**: La API permite todos los or√≠genes (configurable en futuras versiones)
- **Soporte Windows**: No completamente probado; se recomiendan sistemas Unix-like

### Autenticaci√≥n de API

El servidor API soporta autenticaci√≥n opcional mediante el flag `--api-key`:

```bash
# Iniciar servidor con autenticaci√≥n
hfl serve --api-key tu-clave-secreta

# Las peticiones del cliente deben incluir la clave
curl -H "Authorization: Bearer tu-clave-secreta" http://localhost:11434/v1/models
# O
curl -H "X-API-Key: tu-clave-secreta" http://localhost:11434/v1/models
```

## Documentaci√≥n

Documentaci√≥n completa de arquitectura con diagramas disponible:

- **[üìñ Ver Documentaci√≥n de Arquitectura](https://htmlpreview.github.io/?https://github.com/ggalancs/hfl/blob/main/docs/hfl-arquitectura-completa.html)** - Documentaci√≥n HTML interactiva con diagramas de arquitectura, descripciones de m√≥dulos y diagramas de flujo

La documentaci√≥n cubre:
- Arquitectura del sistema y patrones de dise√±o
- Estructura de m√≥dulos y dependencias
- L√≥gica de selecci√≥n de motor de inferencia
- Pipeline de conversi√≥n GGUF
- Caracter√≠sticas de cumplimiento legal
- Referencia de endpoints de API

> **Nota:** La documentaci√≥n tambi√©n est√° disponible en [Ingl√©s](https://htmlpreview.github.io/?https://github.com/ggalancs/hfl/blob/main/docs/hfl-architecture-complete.html).

## Desarrollo

```bash
# Clonar e instalar en modo desarrollo
git clone https://github.com/ggalancs/hfl
cd hfl
pip install -e ".[dev]"

# Ejecutar tests
pytest

# Ejecutar tests con cobertura
pytest --cov=hfl --cov-report=term-missing

# Formatear c√≥digo
ruff format .
ruff check . --fix
```

## Avisos Legales

### Cumplimiento de Exportaci√≥n

hfl solo descarga modelos de pesos abiertos disponibles p√∫blicamente desde HuggingFace Hub. Los usuarios son responsables del cumplimiento de las regulaciones de control de exportaci√≥n aplicables en su jurisdicci√≥n.

hfl no facilita el acceso a pesos de modelos cerrados o controlados para exportaci√≥n.

### Licencias de Modelos

Los modelos descargados a trav√©s de hfl pueden tener sus propias restricciones de licencia. hfl muestra informaci√≥n de licencia antes de la descarga y la almacena con los metadatos del modelo. Los usuarios son responsables de cumplir con las licencias de los modelos.

Las restricciones comunes incluyen:
- **Solo uso no comercial** (CC-BY-NC, MRL)
- **Atribuci√≥n requerida** (Llama, Gemma)
- **Restricciones de uso** (OpenRAIL)

Usa `hfl inspect <modelo>` para ver detalles de licencia de modelos descargados.

### Descargo de Responsabilidad

Los modelos de IA pueden generar contenido inexacto, sesgado o inapropiado. Los usuarios son los √∫nicos responsables de evaluar y usar las salidas del modelo apropiadamente. Ver [DISCLAIMER.md](DISCLAIMER.md) para detalles completos.

## Marcas Registradas

"OpenAI" es una marca registrada de OpenAI, Inc. "Ollama" es una marca registrada de Ollama, Inc. "Hugging Face" y el logo de Hugging Face son marcas registradas de Hugging Face, Inc. Estas marcas se usan aqu√≠ solo con prop√≥sitos de identificaci√≥n.

**hfl es un proyecto independiente y no est√° afiliado, respaldado, ni conectado oficialmente con Hugging Face, Inc., OpenAI, Inc., u Ollama, Inc.** Las referencias a estos servicios describen solo interoperabilidad t√©cnica.

## Licencia

hfl se distribuye como source-available bajo la **hfl Responsible Use License (HRUL) v1.0**.

Esta licencia permite uso libre, modificaci√≥n y distribuci√≥n comercial con una condici√≥n: los trabajos derivados que se distribuyan p√∫blicamente deben mantener las caracter√≠sticas de cumplimiento legal (verificaci√≥n de licencias, descargos de IA, seguimiento de procedencia, protecciones de privacidad y respeto de restricciones).

Eres libre de reescribir, extender, renombrar y vender derivados ‚Äî simplemente no puedes eliminar las caracter√≠sticas de seguridad.

**Nota:** La HRUL no es una licencia open-source aprobada por la OSI. Es una licencia source-available con requisitos de uso responsable, inspirada en Apache 2.0, el copyleft de GPL, y la familia de licencias RAIL para IA.

Ver [LICENSE](LICENSE) para el texto completo y [LICENSE-FAQ.md](LICENSE-FAQ.md) para preguntas frecuentes.
